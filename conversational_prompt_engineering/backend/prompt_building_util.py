import json
import os.path

from genai.schema import ChatRole
from transformers import AutoModel, AutoTokenizer

LLAMA_END_OF_MESSAGE = "<|eot_id|>"

LLAMA_START_OF_INPUT = '<|begin_of_text|>'


# GRANITE_SYSTEM_MESSAGE = 'You are Granite Chat, an AI language model developed by IBM. You are a cautious assistant. You carefully follow instructions. You are helpful and harmless and you follow ethical guidelines and promote positive behavior. You always respond to greetings (for example, hi, hello, g\'day, morning, afternoon, evening, night, what\'s up, nice to meet you, sup, etc) with "Hello! I am Granite Chat, created by IBM. How can I help you today?". Please do not say anything else and do not start a conversation.'

BASELINE_SUMMARIZATION_INSTRUCTION = 'Summarize the main points and key information from the provided text in a concise and clear manner, preserving the original meaning and content.' # this was generated by the prompt : Generate a concise and general prompt for a summarization task in one sentence to Llama-3


class TargetModelHandler:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            with open(os.path.join(os.path.dirname(__file__), "prompt_formats.json"), 'r') as f:
                cls._instance.data = json.load(f)
        return cls._instance

    def get_models(self):
        model_names = self.data.keys()
        model_short_names_and_full_names = [{"full_name": key, "short_name": self.data[key]['short_name']} for key in
                                            model_names]
        return model_short_names_and_full_names

    def format_prompt(self, model, prompt, texts_and_summaries):
        if 'prompt_formats' not in self.data[model]:
            raise Exception(f"prompt format is not defined for model {model}")
        model_vars = self.data[model]['prompt_formats']
        prompt = ''.join([model_vars.get('start_of_input', ''), model_vars.get('system_message', ''),
                          model_vars.get('prompt_prefix', ''), prompt, model_vars.get('prompt_suffix', '')])
        if len(texts_and_summaries) > 0:
            if len(texts_and_summaries) > 1:  # we already have at least two approved summary examples
                prompt += "\n\nHere are some typical text examples and their corresponding desired outputs."
            else:
                prompt += "\n\nHere is an example of a typical text and its desired output."
            for i, item in enumerate(texts_and_summaries):
                if i > 0:
                    prompt += model_vars.get('few_shot_examples_prefix', '')
                text = item['text']
                summary = item['summary']
                prompt += f"\n\n{model_vars.get('input_prefix', '')}{model_vars.get('test_example_placeholder', '').format(text=text)}{model_vars.get('end_of_message', '')}" \
                          f"{model_vars.get('output_prefix', '')}{summary}{model_vars.get('end_of_message', '')}"
            prompt += model_vars.get('test_example_prefix', '')
        prompt += f"{model_vars.get('input_prefix', '')}" + model_vars.get('test_example_placeholder', '') + f"{model_vars.get('end_of_message', '')}"\
                  + f"{model_vars.get('output_prefix', '')}{model_vars.get('end_of_input', '')}"
        return prompt
def _get_llama_header(role):
    return "<|start_header_id|>" + role + "<|end_header_id|>"

#
#
# def build_few_shot_prompt(prompt, texts_and_summaries, model_id):
#     if 'llama' in model_id:
#         return build_few_shot_prompt_llama(prompt, texts_and_summaries)
#     elif 'mixtral' in model_id:
#         return build_few_shot_prompt_mixtral(prompt, texts_and_summaries)
#     elif 'granite' in model_id:
#         return build_few_shot_prompt_granite(prompt, texts_and_summaries)
#     else:
#         raise Exception(f"model {model_id} not supported")
#
#
# def build_few_shot_prompt_granite(prompt, texts_and_summaries):
#     system_prompt = f'<|{ChatRole.SYSTEM}|>\n{GRANITE_SYSTEM_MESSAGE}'
#     prompt = system_prompt + '\n' + f'<|{ChatRole.USER}|>' + '\n' + prompt + '\nYour response should only include the answer. Do not provide any further explanation.'
#     if len(texts_and_summaries) > 0:
#         prompt += "\n\nHere are some examples, complete the last one:\n"
#         for item in texts_and_summaries:
#             text = item['text']
#             summary = item['summary']
#             prompt += f"Text:\n{text}\nSummary:\n{summary}\n\n"
#     else:
#         prompt += "\n\n"
#     prompt += 'Text:\n{text}\nSummary:\n' + f'<|{ChatRole.ASSISTANT}|>'
#     return prompt
# #
#
# def build_few_shot_prompt_mixtral(prompt, texts_and_summaries):
#     prompt = f'[INST] {prompt}\n\n'
#     if len(texts_and_summaries) > 0:
#         if len(texts_and_summaries) > 1:  # we already have at least two approved summary examples
#             prompt += "Here are some typical text examples and their corresponding desired outputs."
#         else:
#             prompt += "Here is an example of a typical text and its desired output."
#         for item in texts_and_summaries:
#             text = item['text']
#             summary = item['summary']
#             prompt += f"\n\nText: {text}\n\nDesired output: {summary}"
#         prompt += "\n\nNow, please generate the desired output for the following text.\n\n"
#     prompt += "Text: {text}\n\nDesired output: [\INST]"
#     return prompt
# #
# #
# def build_few_shot_prompt_llama(prompt, texts_and_summaries):
#     summary_prefix = "Here is a desired output of the text:"
#     prompt = LLAMA_START_OF_INPUT + _get_llama_header(ChatRole.USER) + "\n\n" + prompt + "\n\n"
#     if len(texts_and_summaries) > 0:
#         if len(texts_and_summaries) > 1:  # we already have at least two approved summary examples
#             prompt += "Here are some typical text examples and their corresponding desired outputs."
#         else:
#             prompt += "Here is an example of a typical text and its desired output."
#         for i, item in enumerate(texts_and_summaries):
#             if i > 0:
#                 prompt += _get_llama_header(ChatRole.USER)
#             text = item['text']
#             summary = item['summary']
#             prompt += f"\n\nText: {text}\n\n{LLAMA_END_OF_MESSAGE}" \
#                       f"{_get_llama_header(ChatRole.ASSISTANT)}{summary_prefix}{summary}{LLAMA_END_OF_MESSAGE}"
#         prompt += _get_llama_header(ChatRole.USER) + "\n\nNow, please generate a desired output to the following text.\n\n"
#     prompt += "Text: {text}\n\n" + LLAMA_END_OF_MESSAGE + _get_llama_header(ChatRole.ASSISTANT) + summary_prefix
#     return prompt


def remove_tags_from_zero_shot_prompt(prompt, model_type):
    if model_type == "llama-3":
        return prompt.replace("<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n", "").replace(
            "<|eot_id|><|start_header_id|>assistant<|end_header_id|>", "")
    elif model_type == "mixtral":
        return prompt.replace("[INST] ", ""). replace("[\INST]", "")
    elif model_type == "granite":
        print("Granite prompt is not cleaned up")
        return prompt

if __name__ == "__main__":
    models = TargetModelHandler().get_models()

    # new
    prompt = "summarize this text"
    fs_examples = [{"text": "text_1", "summary": "summary_1"}, {"text": "text_2", "summary": "summary_2"}]

    new_prompt = TargetModelHandler().format_prompt("granite", prompt, fs_examples)
    TargetModelHandler().format_prompt("granite", "summarize this text", [{"text": "text_1", "summary": "summary_1"}, {"text": "text_2", "summary": "summary_2"}])

    # old
    # old_prompt = build_few_shot_prompt_granite(prompt, fs_examples)
    # print([(a,b) for a,b in zip(old_prompt.split(), new_prompt.split()) if a != b])
