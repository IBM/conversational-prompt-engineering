text"AI in commerce: Essential use cases for B2B and B2C. Recent advancements in artificial intelligence (AI) are transforming commerce at an exponential pace. As these innovations are dynamically reshaping the commerce journey, it is crucial for leaders to anticipate and future-proof their enterprises to embrace the new paradigm.  In the context of this rapid advancement, generative AI and automation have the capacity to create more fundamentally relevant and contextually appropriate buying experiences. They can simplify and accelerate workflows throughout the commerce journey, from discovery to the successful completion of a transaction. To take one example, AI-facilitated tools like voice navigation promise to upend the way users fundamentally interact with a system. And these technologies provide brands with intelligent tools, enabling more productivity and efficiency than was possible even five years ago. AI models analyze vast amounts of data quickly, and get more accurate by the day. They can provide valuable insights and forecasts to inform organizational decision-making in omnichannel commerce, enabling businesses to make more informed and data-driven decisions. By implementing effective AI solutions—using traditional and generative AI—brands can create seamless and personalized buying experiences. These experiences result in increased customer loyalty, customer engagement, retention, and increased share of wallet across both business-to-business (B2B) and business-to-consumer (B2C) channels. Ultimately, they drive significant increases in conversions driving meaningful revenue growth from the transformed commerce experience.  Creating seamless experiences for skeptical users: It’s been a swift shift toward a ubiquitous use of AI. Early iterations of e-commerce used traditional AI largely to create dynamic marketing campaigns, improve the online shopping experience, or triage customer requests. Today the technology’s advanced capabilities encourage widespread adoption. AI can be integrated into every touchpoint across the commerce journey. According to a recent report from the IBM Institute for Business Value, half of CEOs are integrating generative AI into products and services. Meanwhile, 43% are using the technology to inform strategic decisions.  But customers aren’t yet completely on board. Fluency with AI has grown along with the rollout of ChatGPT and virtual assistants like Amazon’s Alexa. But as businesses around the globe rapidly adopt the technology to augment processes from merchandising to order management, there is some risk. High-profile failures and expensive litigation threatens to sour public opinion and cripple the promise of generative AI-powered commerce technology.  Generative AI’s impact on the social media landscape garners occasional bad press. Disapproval of brands or retailers that use AI is as high as 38% among older generations, requiring businesses to work harder to gain their trust. A report from the IBM Institute of Business Value found that there’s enormous room for improvement in the customer experience. Only 14% of surveyed consumers described themselves as “satisfied” with their experience purchasing goods online. A full one-third of consumers found their early customer support and chatbot experiences that use natural language processing (NLP) so disappointing that they didn’t want to engage with the technology again. And the centrality of these experiences isn’t limited to B2C vendors. Over 90% of business buyers say a company’s customer experience is as important as what it sells. Poorly run implementations of traditional or generative AI technology in commerce—such as deploying deep learning models trained on inadequate or inappropriate data—lead to bad experiences that alienate both consumers and businesses. To avoid this, it’s crucial for businesses to carefully plan and design intelligent automation initiatives that prioritize the needs and preferences of their customers, whether they are consumers or B2B buyers. By doing so, brands can create contextually relevant personalized buying experiences, seamless and friction-free, which foster customer loyalty and trust. This article explores four transformative use cases for AI in commerce that are already enhancing the customer journey, especially in the e-commerce business and e-commerce platform components of the overall omnichannel experience. It also discusses how forward-thinking companies can effectively integrate AI algorithms to usher in a new era of intelligent commerce experiences for both consumers and brands. But none of these use cases exist in a vacuum. As the future of commerce unfolds, each use case interacts holistically to transform the customer journey from end-to-end–for customers, for employees, and for their partners.  Use case 1: AI for modernization and business model expansion: AI-powered tools can be incredibly valuable in optimizing and modernizing business operations throughout the customer journey, but it is critical in the commerce continuum. By using machine learning algorithms and big data analytics, AI can uncover patterns, correlations and trends that might escape human analysts. These capabilities can help businesses make informed decisions, improve operational efficiencies, and identify opportunities for growth. The applications of AI in commerce are vast and varied. They include: Dynamic content: Traditional AI fuels recommendation engines that suggest products based on customer purchase history and customer preferences, creating personalized experiences that result in increased customer satisfaction and loyalty. Experience building strategies like these have been  used by online retailers for years. Today, generative AI enables dynamic customer segmentation and profiling. This segmentation activates personalized product recommendations and suggestions, such as product bundles and upsells, that adapt to individual customer behavior and preferences, resulting in higher engagement and conversion rates. Commerce operation: Traditional AI allows for the automation of routine tasks such as inventory management, order processing and fulfillment optimization, resulting in increased efficiency and cost savings. Generative AI activates predictive analytics and forecasting, enabling businesses to anticipate and respond to changes in demand, reducing stockouts and overstocking, and improving supply chain resilience. It can also significantly impact real-time fraud detection and prevention, minimizing financial losses and improving customer trust.  Business model expansion: Both traditional and generative AI have pivotal and functions that can redefine business models. They can, for example, enable the seamless integration of a marketplace platform where AI-driven algorithms match supply with demand, effectively connecting sellers and buyers across different geographic areas and market segments. Generative AI can also enable new forms of commerce—such as voice commerce, social commerce and experiential commerce—that provide customers with seamless and personalized shopping experiences.Traditional AI can enhance international purchasing by automating tasks such as currency conversions and tax calculations. It can also facilitate compliance with local regulations, streamlining the logistics of cross-border transactions. However, generative AI can create value by generating multilingual support and personalized marketing content. These tools adapt content to the cultural and linguistic nuances of different regions, offering a more contextually relevant experience for international customers and consumers. Use case 2: AI for dynamic product experience management (PXM): Using the power of AI, brands can revolutionize their product experience management and user experience by delivering personalized, engaging and seamless experiences at every touchpoint in commerce. These tools can manage content, standardize product information, and drive personalization. With AI, brands can create a product experience that informs, validates and builds the confidence necessary for conversion. Some ways to use relevant personalization by transforming product experience management include: Intelligent content management. Generative AI can revolutionize content management by automating the creation, classification and optimization of product content. Unlike traditional AI, which analyzes and categorizes existing content, generative AI can create new content tailored to individual customers. This content includes product descriptions, images, videos and even interactive experiences. By using generative AI, brands can save time and resources while simultaneously delivering high-quality, engaging content that resonates with their target audience. Generative AI can also help brands maintain consistency across all touchpoints, ensuring that product information is accurate, up-to-date and optimized for conversions. Hyperpersonalization: Generative AI can take personalization to the next level by creating customized experiences that are tailored to individual customers. By analyzing customer data and customer queries, generative AI can create personalized product recommendations, offers and content that are more likely to drive conversions. Unlike traditional AI, which can only segment customers based on predefined criteria, generative AI can create unique experiences for each customer, considering their preferences, behavior and interests. Such personalization is crucial as organizations adopt software-as-a-service (SaaS) models more frequently: Global subscription-model billing is expected to double over the next six years, and most consumers say those models help them feel more connected to a business. With AI’s potential for hyperpersonalization, those subscription-based consumer experiences can vastly improve. These experiences result in higher engagement, increased customer satisfaction, and ultimately, higher sales.  Experiential product information: Al tools allow individuals to learn more about products through processes like visual search, taking a photograph of an item to learn more about it. Generative AI takes these capabilities further, transforming product information by creating interactive, immersive experiences that help customers better understand products and make informed purchasing decisions. For example, generative AI can create 360-degree product views, interactive product demos, and virtual try-on capabilities. These experiences provide a richer product understanding and help brands differentiate themselves from competitors and build trust with potential customers. Unlike traditional AI, which provides static product information, generative AI can create engaging, memorable experiences that drive conversions and build brand loyalty.  Smart search and recommendations: Generative AI can revolutionize search engines and recommendations by providing customers with personalized, contextualized results that match their intent and preferences. Unlike traditional AI, which relies on keyword matching, generative AI can understand natural language and intent, providing customers with relevant results that are more likely to match their search queries. Generative AI can also create recommendations that are based on individual customer behavior, preferences and interests, resulting in higher engagement and increased sales. By using generative AI, brands can deliver intelligent search and recommendation capabilities that enhance the overall product experience and drive conversions.  Use case 3: AI for order intelligence. Generative AI and automation can allow businesses to make data-driven decisions to streamline processes across the supply chain, reducing inefficiency and waste. For example, a recent analysis from McKinsey found that nearly 20% of logistics costs could stem from “blind handoffs”—the moment a shipment is dropped at some point between the manufacturer and its intended location. According to the McKinsey report, these inefficient interactions might amount to as much as $95 billion in losses in the United States every year. AI-powered order intelligence can reduce some of these inefficiencies by using: Order orchestration and fulfillment optimization: By considering factors such as inventory availability, location proximity, shipping costs and delivery preferences, AI tools can dynamically select the most cost-effective and efficient fulfillment options for an individual order. These tools might dictate the priority of deliveries, predict order routing, or dispatch deliveries to comply with sustainability requirements.   Demand forecasting: By analyzing historical data, AI can predict demand and help businesses optimize their inventory levels and minimize excess, reducing costs and improving efficiency. Real-time inventory updates allow businesses to adapt quickly to changing conditions, allowing for effective resource allocation. Inventory transparency and order accuracy: AI-powered order management systems provide real-time visibility into all aspects of the critical order management workflow. These tools enable companies to proactively identify potential disruptions and mitigate risks. This visibility helps customers and consumers trust that their orders will be delivered exactly when and how they were promised. Use case 4: AI for payments and security. Intelligent payments enhance the payment and security process, improving efficiency and accuracy. Such technologies can help process, manage and secure digital transactions—and provide advance warning of potential risks and the possibility of fraud.  Intelligent payments: Traditional and generative AI both enhance transaction processes for B2C and B2B customers making purchases in online stores. Traditional AI optimizes POS systems, automates new payment methods, and facilitates multiple payment solutions across channels, streamlining operations and improving consumer experiences. Generative AI creates dynamic payment models for B2B customers, addressing their complex transactions with customized invoicing and predictive behaviors. The technology can also provide strategic and personalized financial solutions. Also, generative AI can enhance B2C customer payments by creating personalized and dynamic pricing strategies. Risk management and fraud detection: Traditional AI and machine learning excel in processing vast volumes of B2C and B2B payments, enabling businesses to identify and respond to suspicious trends swiftly. Traditional AI automates the detection of irregular patterns and potential fraud, reducing the need for costly human analysis. Meanwhile, generative AI contributes by simulating various fraud scenarios to predict and prevent new types of fraudulent activities before they occur, enhancing the overall security of payment systems. Compliance and data privacy: In the commerce journey, traditional AI helps secure transaction data and automates compliance with payment regulations, enabling businesses to quickly adapt to new financial laws and conduct ongoing audits of payment processes. Generative AI further enhances these capabilities by developing predictive models that anticipate changes in payment regulations. It can also automate intricate data privacy measures, helping businesses to maintain compliance and protect customer data efficiently. The future of AI in commerce is based on trust: Today’s commercial landscape is swiftly transforming into a digitally interconnected ecosystem. In this reality, the integration of generative AI across omnichannel commerce—both B2B and B2C—is essential. However, for this integration to be successful, trust must be at the core of its implementation. Identifying the right moments in the commerce journey for AI integration is also crucial. Companies need to conduct comprehensive audits of their existing workflows to make sure AI innovations are both effective and sensitive to consumer expectations. Introducing AI solutions transparently and with robust data security measures is imperative. Businesses must approach the introduction of trusted generative AI as an opportunity to enhance the customer experience by making it more personalized, conversational and responsive. This requires a clear strategy that prioritizes human-centric values and builds trust through consistent, observable interactions that demonstrate the value and reliability of AI enhancements.  Looking forward, trusted AI redefines customer interactions, enabling businesses to meet their clients precisely where they are, with a level of personalization previously unattainable. By working with AI systems that are reliable, secure and aligned with customer needs and business outcomes, companies can forge deeper, trust-based relationships. These relationships are essential for long-term engagement and will be essential to every business’s future commerce success, growth and, ultimately, their viability.""Enhancing triparty repo transactions with IBM MQ for efficiency, security and scalability -- The exchange of securities between parties is a critical aspect of the financial industry that demands high levels of security and efficiency. Triparty repo dealing systems, central to these exchanges, require seamless and secure communication across different platforms. The Clearing Corporation of India Limited (CCIL) recently recommended (link resides outside ibm.com) IBM® MQ as the messaging software requirement for all its members to manage the triparty repo dealing system. IBM MQ and its effect on triparty repo dealing system - IBM MQ is a messaging system that allows parties to communicate with each other in a protected and reliable manner. In a triparty repo dealing system, IBM MQ acts as the backbone of communication, enabling the parties to exchange information and instructions related to the transaction. IBM MQ enhances the efficiency of a triparty repo dealing system across various factors: 1. Efficient communication: IBM MQ enables efficient communication between parties, allowing them to exchange information and instructions in real-time. This reduces the risk of errors and miscommunications, which can lead to significant losses in the financial industry. With IBM MQ, parties can make sure that transactions are executed accurately and efficiently. IBM MQ makes sure that the messages are delivered exactly once, and this aspect is particularly important in the financial industry. 2. Scalable and can handle more messages: IBM MQ is designed to handle a large volume of messages, making it an ideal solution for triparty repo dealing systems. As the system grows, IBM MQ can scale up to meet the increasing demands of communication, helping the system remain efficient and reliable. 3. Robust security: IBM MQ provides a safe communication channel between parties, protecting sensitive information from unauthorized access. This is critical in the financial industry, where security is paramount. IBM MQ uses encryption and other security measures to protect data, so that transactions are conducted safely and securely. 4. Flexible and easy to integrate: IBM MQ is a flexible messaging system that can be seamlessly integrated with other systems and applications. This makes it easy to incorporate new features and functionalities into the triparty repo dealing system, allowing it to adapt to changing market conditions and customer needs. How to use IBM MQ effectively in triparty repo dealing systems - Follow these guidelines to use IBM MQ effectively in a triparty repo dealing system and make a difference: 1. Define clear message formats for different types of communications, such as trade capture, confirmation and settlement. This will make sure that parties understand the structure and content of messages, reducing errors and miscommunications. 2. Implement strong security measures to protect sensitive information, such as encryption and access controls. This will protect the data  from unauthorized access and tampering. 3. Monitor message queues to verify that messages are being processed efficiently and that there are no errors or bottlenecks. This will help identify issues early, reducing the risk of disruptions to the system. 4. Use message queue management tools to manage and monitor message queues. These tools can help optimize message processing, reduce latency and improve system performance. 5. Test and validate messages regularly to ensure that they are formatted correctly and that the information is accurate. This will help reduce errors and miscommunications, enabling transactions to be executed correctly. CCIL as triparty repo dealing system and IBM MQ - The Clearing Corporation of India Ltd. (CCIL) is a central counterparty (CCP) that was set up in April 2001 to provide clearing and settlement for transactions in government securities, foreign exchange and money markets in the country. CCIL acts as a central counterparty in various segments of the financial markets regulated by the Reserve Bank of India (RBI), namely., the government securities segment, that is, outright, market repo and triparty repo, USD-INR and forex forward segments. As recommended by CCIL, all members are required to use IBM MQ as the messaging software for the triparty repo dealing system. IBM MQ v9.3 Long Term Support (LTS) release and above is the recommended software to have in the members’ software environment. IBM MQ plays a critical role in triparty repo dealing systems, enabling efficient, secure, and reliable communication between parties. By following the guidelines outlined above, parties can effectively use IBM MQ to facilitate smooth and secure transactions. As the financial industry continues to evolve, the importance of IBM MQ in triparty repo dealing systems will only continue to grow, making it an essential component of the system. ""Enhance your data security posture with a no-code approach to application-level encryption -- Data is the lifeblood of every organization. As your organization’s data footprint expands across the clouds and between your own business lines to drive value, it is essential to secure data at all stages of the cloud adoption and throughout the data lifecycle. While there are different mechanisms available to encrypt data throughout its lifecycle (in transit, at rest and in use), application-level encryption (ALE) provides an additional layer of protection by encrypting data at its source. ALE can enhance your data security, privacy and sovereignty posture. Why should you consider application-level encryption? Figure 1 illustrates a typical three-tier application deployment, where the application back end is writing data to a managed Postgres instance. If you look at the high-level data flow, data originates from the end user and is encrypted in transit to the application, between application microservices (UI and back end), and from the application to the database. Finally, the database encrypts the data at rest using either bring your own key ( or keep your own key ( strategy. In this deployment, both runtime and database admins are inside the trust boundary. This means you’re assuming no harm from these personas. However, as analysts and industry experts point out, there is a human element at the root of most cybersecurity breaches. These breaches happen through error, privilege misuse or stolen credentials and this risk can be mitigated by placing these personas outside the trust boundary. So, how can we enhance the security posture by efficiently placing privileged users outside the trust boundary? The answer lies in application-level encryption. How does application-level encryption protect from data breaches? Application-level encryption is an approach to data security where we encrypt the data within an application before it is stored or transmitted through different parts of the system. This approach significantly reduces the various potential attack points by shrinking the data security controls right down to the data. By introducing ALE to the application, as shown in figure 2, we help ensure that data is encrypted within the application. It remains encrypted for its lifecycle thereon, until it is read back by the same application in question. This helps make sure that privileged users on the database front (such as database administrators and operators) are outside the trust boundary and cannot access sensitive data in clear text. However, this approach requires changes to the application back end, which places another set of privileged users (ALE service admin and security focal) inside the trust boundary. It can be difficult to confirm how the encryption keys are managed in the ALE service. So, how are we going to bring the value of ALE without such compromises? The answer is through a data security broker. Why should you consider Data Security Broker? IBM Cloud® Security and Compliance Center (SCC) Data Security Broker (DSB) provides an application-level encryption software with a no-code change approach to seamlessly mask, encrypt and tokenize data. It enforces a role-based access control (RBAC) with field and column level granularity. DSB has two components: a control plane component called DSB Manager and a data plane component called DSB Shield, as shown in Figure 3. DSB Manager (the control plane) is not in the data path and is now running outside the trust boundary. DSB Shield (the data plane component) seamlessly retrieves the policies such as encryption, masking, RBAC and uses the customer-owned keys to enforce the policy with no-code changes to the application! Data Security Broker offers these benefits: 1. Security: Personally identifiable information (PII) is anonymized before ingestion to the database and is protected even from database and cloud admins. 2. Ease: The data is protected where it flows, without code changes to the application. 3. Efficiency: DSB supports scaling and to the end user of the application, this results in no perceived impact on application performance. 4. Control: DSB offers customer-controlled key management access to data. Help to avoid the risk of data breaches - Data breaches come with the high cost of time-to-address, the risk of industry and regulatory compliance violations and associated penalties, and the risk of loss of reputation. Mitigating these risks is often time-consuming and expensive due to the application changes required to secure sensitive data, as well as the oversight required to meet compliance requirements. Making sure your data protection posture is strong  helps avoid the risk of breaches. IBM Cloud Security and Compliance Center Data Security Broker provides the IBM Cloud and hybrid-multicloud with IBM Cloud Satellite® no-code application-level encryption  to protect your application data and enhance your security posture toward zero trust guidelines.""How will quantum impact the biotech industry? The physics of atoms and the technology behind treating disease might sound like disparate fields. However, in the past few decades, advances in artificial intelligence, sensing, simulation and more have driven enormous impacts within the biotech industry. Quantum computing provides an opportunity to extend these advancements with computational speedups and/or accuracy in each of those areas. Now is the time for enterprises, commercial organizations and research institutions to begin exploring how to use quantum to solve problems in their respective domains. As a Partner in IBM’s Quantum practice, I’ve had the pleasure of working alongside Wade Davis, Vice President of Computational Science & Head of Digital for Research at Moderna, to drive quantum innovation in healthcare. Below, you’ll find some of the perspectives we share on the future in quantum compute in biotech. What is quantum computing? Quantum computing is a new kind of computer processing technology that relies on the science that governs the behavior of atoms to solve problems that are too complex or not practical for today’s fastest supercomputers. We don’t expect quantum to replace classical computing. Rather, quantum computers will serve as a highly specialized and complementary computing resource for running specific tasks. A classical computer is how you’re reading this blog. These computers represent information in strings of zeros and ones and manipulate these strings by using a set of logical operations. The result is a computer that behaves deterministically—these operations have well-defined effects, and a sequence of operations resulting in a single outcome. Quantum computers, however, are probabilistic—the same sequence of operations can have different outcomes, allowing these computers to explore and calculate multiple scenarios simultaneously. But this alone does not explain the full power of quantum computing. Quantum mechanics offers us access to a tweaked and counterintuitive version of probability that allows us to run computations inaccessible to classical computers. Therefore, quantum computers enable us to evaluate new dimensions for existing problems and explore entirely new frontiers that are not accessible today. And they perform computations in a way that more closely mirrors nature itself. As mentioned, we don’t expect quantum computers to replace classical computers. Each one has its strengths and weaknesses: while quantum will excel at running certain algorithms or simulating nature, classical will still take on much of the work. We anticipate a future wherein programs weave quantum and classical computation together, relying on each one where they’re more appropriate. Quantum will extend the power of classical. Unlocking new potential - A set of core enterprise applications has crystallized from an environment of rapidly maturing quantum hardware and software. What the following problems share are many variables, a structure that seems to map well to the rules of quantum mechanics, and difficulty solving them with today’s HPC resources. They broadly fall into three buckets: 1. Advanced mathematics and complex data structures. The multidimensional nature of quantum mechanics offers a new way to approach problems with many moving parts, enabling better analytic performance for computationally complex problems. Even with recent and transformative advancements in AI and generative AI, quantum compute promises the ability to identify and recognize patterns that are not detectable for classical-trained AI, especially where data is sparse and imbalanced. For biotech, this might be beneficial for combing through datasets to find trends that might identify and personalize interventions that target disease at the cellular level. 2. Search and optimization. Enterprises have a large appetite for tackling complex combinatorial and black-box problems to generate more robust insights for strategic planning and investments. Though further on the horizon, quantum systems are being intensely studied for their ability to consider a broad set of computations concurrently, by generating statistical distributions, unlocking a host of promising opportunities including the ability to rapidly identify protein folding structures and optimize sequencing to advance mRNA-based therapeutics. 3. Simulating nature. Quantum computers naturally re-create the behavior of atoms and even subatomic particles—making them valuable for simulating how matter interacts with its environment. This opens up new possibilities to design new drugs to fight emerging diseases within the biotech industry—and more broadly, to discover new materials that can enable carbon capture and optimize energy storage to help industries fight climate change. At IBM, we recognize that our role is not only to provide world-leading hardware and software, but also to connect quantum experts with nonquantum domain experts across these areas to bring useful quantum computing sooner. To that end, we convened five working groups covering healthcare/life sciences, materials science, high-energy physics, optimization and sustainability. Each of these working groups gathers in person to generate ideas and foster collaborations—and then these collaborations work together to produce new research and domain-specific implementations of quantum algorithms. As algorithm discovery and development matures and we expand our focus to real-world applications, commercial entities, too, are shifting from experimental proof-of-concepts toward utility-scale prototypes that will be integrated into their workflows. Over the next few years, enterprises across the world will be investing to upskill talent and prepare their organizations for the arrival of quantum computing. Among industries that are making the pivot to useful quantum computing, the biotech industry is moving rapidly to explore how quantum compute can help reduce the cost and speed up the time required to discover, create, and distribute therapeutic treatments that will improve the health, the well being and the quality of life for individuals suffering from chronic disease. According to BCG’s Quantum Computing Is Becoming Business Ready report: “eight of the top ten biopharma companies are piloting quantum computing, and five have partnered with quantum providers.” Partnering with IBM - Recent advancements in quantum computing have opened new avenues for tackling complex combinatorial problems that are intractable for classical computers. Among these challenges, the prediction of mRNA secondary structure is a critical task in molecular biology, impacting our understanding of gene expression, regulation and the design of RNA-based therapeutics. For example, Moderna has been pioneering the development of quantum for biotechnology. Emerging from the pandemic, Moderna established itself as a game-changing innovator in biotech when a decade of extensive R&D allowed them to use their technology platform to deliver a COVID-19 vaccine with record speed. Given the value of their platform approach, perhaps quantum might further push their ability to perform mRNA research, providing a host of novel mRNA vaccines more efficiently than ever before. This is where IBM can help. As an initial step, Moderna is working with IBM to benchmark the application of quantum computing against a classical CPlex protein analysis solver. They’re evaluating the performance of a quantum algorithm called CVaR VQE on randomly generated mRNA nucleotide sequences to accurately predict stable mRNA structures as compared to current state of the art. Their findings demonstrate the potential of quantum computing to provide insights into mRNA dynamics and offer a promising direction for advancing computational biology through quantum algorithms. As a next step, they hope to push quantum to sequence lengths beyond what CPLEX can handle. This is just one of many collaborations that are transforming biotech processes with the help of quantum computation. Biotech enterprises are using IBM Quantum Systems to run their workloads on real utility-scale quantum hardware, while leveraging the IBM Quantum Network to share expertise across domains. And with our updated IBM Quantum Accelerator program, enterprises can now prepare their organizations with hands-on guidance to identify use cases, design workflows and develop utility-scale prototypes that use quantum computation for business impact. The time has never been better to begin your quantum journey—get started today.""Achieving cloud excellence and efficiency with cloud maturity models – 1. Cloud maturity models (CMMs) are helpful tools for evaluating an organization’s cloud adoption readiness and cloud security posture. 2. Cloud adoption presents tremendous business opportunity—to the tune of USD 3 trillion—and more mature cloud postures drive greater cloud ROI and more successful digital transformations. 3. There are many CMMs in practice and organizations need to decide which are most appropriate for their business and their needs. CMMs can be used individually, or in conjunction with one another. Why move to the cloud? Business leaders worldwide are asking their teams the same question: “Are we using the cloud effectively?” This quandary often comes with an accompanying worry: “Are we spending too much money on cloud computing?” Given the statistics—82% of surveyed respondents in a 2023 Statista study cited managing cloud spend as a significant challenge—it’s a legitimate concern. Concerns around security, governance and lack of resources and expertise also top the list of respondents’ concerns. Cloud maturity models are a useful tool for addressing these concerns, grounding organizational cloud strategy and proceeding confidently in cloud adoption with a plan. Cloud maturity models (or CMMs) are frameworks for evaluating an organization’s cloud adoption readiness on both a macro and individual service level. They help an organization assess how effectively it is using cloud services and resources and how cloud services and security can be improved. Why move to cloud? Organizations face increased pressure to move to the cloud in a world of real-time metrics, microservices and APIs, all of which benefit from the flexibility and scalability of cloud computing. An examination of cloud capabilities and maturity is a key component of this digital transformation and cloud adoption presents tremendous upside. McKinsey believes it presents a USD 3 trillion opportunity and nearly all of responding cloud leaders  (99%) view the cloud as the cornerstone of their digital strategy, according to a Deloitte study. A successful cloud strategy requires a comprehensive assessment of cloud maturity. This assessment is used to identify the actions—such as upgrading legacy tech and adjusting organizational workflows—that the organization needs to take to fully realize cloud benefits and pinpoint current shortcomings. CMMs are a great tool for this assessment. There are many CMMs in practice and organizations must decide what works best for their business needs. A good starting point for many organizations is to engage in a three-phase assessment of cloud maturity using the following models: a cloud adoption maturity model, a cloud security maturity model and a cloud-native maturity model. Cloud adoption maturity model - This maturity model helps measure an organization’s cloud maturity in aggregate. It identifies the technologies and internal knowledge that an organization has, how suited its culture is to embrace managed services, the experience of its DevOps team, the initiatives it can begin to migrate to cloud and more. Progress along these levels is linear, so an organization must complete one stage before moving to the next stage. 1. Legacy: Organizations at the beginning of their journey will have no cloud-ready applications or workloads, cloud services or cloud infrastructure. Ad hoc: Next is ad hoc maturity, which likely means the organization has begun its journey through cloud technologies like infrastructure as a service (IaaS), the lowest-level control of resources in the cloud. IaaS customers receive compute, network and storage resources on an on-demand, over the internet, pay-as-you-go pricing basis. 2. Repeatable: Organizations at this stage have begun to make more investments in the cloud. This might include establishing a Cloud Center of Excellence (CCoE) and examining the scalability of initial cloud investments. Most importantly, the organization has now created repeatable processes for moving apps, workstreams and data to the cloud. 3. Optimized: Cloud environments are now working efficiently and every new use case follows the same foundation set forth by the organdization. 4. Cloud-advanced: The organization now has most, if not all, of its workstreams on the cloud. Everything runs seamlessly and efficiently and all stakeholders are aware of the cloud’s potential to drive business objectives. Cloud security maturity model - The optimization of security is paramount for any organization that moves to the cloud. The cloud can be more secure than on-premises data centers, thanks to robust policies and postures used by cloud providers. Prioritizing cloud security is important considering that public cloud-based breaches often take months to correct and can have serious financial and reputational consequences. Cloud security represents a partnership between the cloud service provider (CSP) and the client. CSPs provide certifications on the security inherent in their offerings, but clients that build in the cloud can introduce misconfigurations or other issues when they build on top of the cloud infrastructure. So CSPs and clients must work together to create and maintain secure environments. The Cloud Security Alliance, of which IBM® is a member, has a widely adopted cloud security maturity model (CSMM). The model provides good foundation for organizations looking to better embed security into their cloud environments. Organizations may not want or need to adopt the entire model, but can use whichever components make sense. The model’s five stages revolve around the organization’s level of security automation. 1. No automation: Security professionals identify and address incidents and problems manually through dashboards. 2. Simple SecOps: This phase includes some infrastructure-as-code (IaC) deployments and federation on some accounts. 3. Manually executed scripts: This phase incorporates more federation and multi-factor authentication (MFA), although most automation is still executed manually. 4. Guardrails: It includes a larger library of automation expanding into multiple account guardrails, which are high-level governance policies for the cloud environment. 5. Automation everywhere: This is when everything is integrated into IaC and MFA and federation usage is pervasive. Cloud-native maturity models - The first two maturity models refer more to an organization’s overall readiness; the cloud-native maturity model (CNMM) is used to evaluate an organization’s ability to create apps (whether built internally or through open source tooling) and workloads that are cloud-native. According to Deloitte, 87% of cloud leaders embrace cloud-native development. As with other models, business leaders should first understand their business goals before diving into this model. These objectives will help determine what stage of maturity is necessary for the organization. Business leaders also need to look at their existing enterprise applications and decide which cloud migration strategy is most appropriate. Most “lifted and shifted” apps can operate in a cloud environment but might not to reap the full benefits of cloud. Cloud mature organizations often decide it’s most effective to build cloud-native applications for their most important tools and services. The Cloud Native Computing Foundation has put forth its own model. Level 1 – Build: An organization is in pre-production related to one proof of concept (POC) application and currently has limited organizational support. Business leaders understand the benefits of cloud native and, though new to the technology, team members have basic technical understanding. Level 2 – Operate: Teams are investing in training and new skills and SMEs are emerging within the organization. A DevOps practice is being developed, bringing together cloud engineers and developer groups. With this organizational change, new teams are being defined, agile project groups created and feedback and testing loops established. Level 3 – Scale: Cloud-native strategy is now the preferred approach. Competency is growing, there is increased stakeholder buy-in and cloud-native has become a primary focus. The organization is beginning to implement shift-left policies and actively training all employees on security initiatives. This level is often characterized by a high degree of centralization and clear delineation of responsibilities, however bottlenecks in the process emerge and velocity might decrease. Level 4 – Improve: At level 4, the cloud is the default infrastructure for all services. There is full commitment from leadership and team focus revolves heavily around cloud cost optimization. The organization explores areas to improve and processes that can be made more efficient. Cloud expertise and responsibilities are shifting from developers to all employees through self-service tools. Multiple groups have adopted Kubernetes for deploying and managing containerized applications.  With a strong, established platform, the decentralization process can begin in earnest. Level 5 – Optimize: At this stage, the business has full trust in the technology team and employees company-wide are onboarded to the cloud-native environment. Service ownership is established and distributed to self-sufficient teams. DevOps and DevSecOps are operational, highly skilled and fully scaled. Teams are comfortable with experimentation and skilled in using data to inform business decisions. Accurate data practices boost optimization efforts and enables the organization to further adopt FinOps practices. Operations are smooth, goals outlined in the initial phase have been achieved and the organization has a flexible platform that suits its needs. What’s best for my organization? An organization’s cloud maturity level dictates which benefits and to what degree it stands to gain from a move to the cloud. Not every organization will reach, or want to reach, the top level of maturity in each, or all, of the three models discussed here. However, it’s likely that organizations will find it difficult to compete without some level of cloud maturity, since 70% of workloads will be on the cloud by 2024, according to Gartner. The more mature an organization’s cloud infrastructure, security and cloud-native application posture, the more the cloud becomes advantageous. With a thorough examination of current cloud capabilities and a plan to improve maturity moving forward, an organization can increase the efficiency of its cloud spend and maximize cloud benefits. Advancing cloud maturity with IBM - Cloud migration with IBM® Instana® Observability helps set organizations up for success at each phase of the migration process (plan, migrate, run) to make sure that applications and infrastructure run smoothly and efficiently. From setting performance baselines and right-sizing infrastructure to identifying bottlenecks and monitoring the end-user experience, Instana provides several solutions that help organizations create more mature cloud environments and processes.  However, migrating applications, infrastructure and services to cloud is not enough to drive a successful digital transformation. Organizations need an effective cloud monitoring strategy that uses robust tools to track key performance metrics—such as response time, resource utilization and error rates—to identify potential issues that could impact cloud resources and application performance. Instana provides comprehensive, real-time visibility into the overall status of cloud environments. It enables IT teams to proactively monitor and manage cloud resources across multiple platforms, such as AWS, Microsoft Azure and Google Cloud Platform. The IBM Turbonomic® platform proactively optimizes the delivery of compute, storage and network resources across stacks to avoid overprovisioning and increase ROI. Whether your organization is pursuing a cloud-first, hybrid cloud or multicloud strategy, the Turbonomic platform’s AI-powered automation can help contain costs while preserving performance with automatic, continuous cloud optimization."