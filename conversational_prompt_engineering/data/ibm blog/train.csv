text"How AI-powered recruiting helps Spain’s leading soccer team score -- Phrases like “striking the post” and “direct free kick outside the 18” may seem foreign if you’re not a fan of football (for Americans, see: soccer). But for a football scout, it’s the daily lexicon of the job, representing crucial language that helps assess a player’s value to a team. And now, it’s also the language spoken and understood by Scout Advisor—an innovative tool using natural language processing (NLP) and built on the IBM® watsonx™ platform especially for Spain’s Sevilla Fútbol Club. On any given day, a scout has several responsibilities: observing practices, talking to families of young players, taking notes on games and recording lots of follow-up paperwork. In fact, paperwork is a much more significant part of the job than one might imagine. As Victor Orta, Sevilla FC Sporting Director, explained at his conference during the World Football Summit in 2023: “We are never going to sign a player with data alone, but we will never do it without resorting to data either. In the end, the good player will always have good data, but then there is always the human eye, which is the one that must evaluate everything and decide.” Benched by paperwork: Back in 2021, an avalanche of paperwork plagued Sevilla FC, a top-flight team based in Andalusia, Spain. With an elite scouting team featuring 20-to-25 scouts, a single player can accumulate up to 40 scout reports, requiring 200-to-300 hours of review. Overall, Sevilla FC was tasked with organizing more than 200,000 total reports on potential players—an immensely time-consuming job. Combining expert observation alongside the value of data remained key for the club. Scout reports look at the quantitative data of game-time minutiae, like scoring attempts, accurate pass percentages, assists, as well as qualitative data like a player’s attitude and alignment with team philosophy. At the time, Sevilla FC could efficiently access and use quantitative player data in a matter of seconds, but the process of extracting qualitative information from the database was much slower in comparison. In the case of Sevilla FC, using big data to recruit players had the potential to change the core business. Instead of scouts choosing players based on intuition and bias alone, they could also use statistics, and confidently make better business decisions on multi-million-dollar investments (that is, players). Not to mention, when, where and how to use said players. But harnessing that data was no easy task. Getting the IBM assist: Sevilla FC takes data almost as seriously as scoring goals. In 2021, the club created a dedicated data department specifically to help management make better business decisions. It has now grown to be the largest data department in European football, developing its own AI tool to help track player movements through news coverage, as well as internal ticketing solutions. But when it came to the massive amount of data collected by scouters, the department knew it had a challenge that would take a reliable partner. Initially, the department consulted with data scientists at the University of Sevilla to develop models to organize all their data. But soon, the club realized it would need more advanced technology. A cold call from an IBM representative was fortuitous. “I was contacted by [IBM Client Engineering Manager] Arturo Guerrero to know more about us and our data projects,” says Elias Zamora, Sevilla FC chief data officer. “We quickly understood there were ways to cooperate. Sevilla FC has one of the biggest scouting databases in the professional football, ready to be used in the framework of generative AI technologies. IBM had just released watsonx, its commercial generative AI and scientific data platform based on cloud. Therefore, a partnership to extract the most value from our scouting reports using AI was the right initiative.”  Coordinating the play: Sevilla FC connected with the IBM Client Engineering team to talk through its challenges and a plan was devised. Because Sevilla FC was able to clearly explain its challenges and goals—and IBM asked the right questions—the technology soon followed. The partnership determined that IBM watsonx.ai™ would be the best solution to quickly and easily sift through a massive player database using foundation models and generative AI to process prompts in natural language. Using semantic language for search provided richer results: for instance, a search for “talented winger” translated to “a talented winger is capable of taking on defenders with dribbling to create space and penetrate the opposition’s defense.”  The solution—titled Scout Advisor—presents a curated list of players matching search criteria in a well-designed, user-friendly interface. Its technology helps unlock the entire potential of the Sevilla FC’s database, from the intangible impressions of a scout to specific data assets. Scoring the goal : Scout Advisor’s pilot program went into production in January 2024, and is currently training with 200,000 existing reports. The club’s plan is to use the tool during the summer 2024 recruiting season and see results in September. So far, the reviews have been positive.  “Scout Advisor has the capability to revolutionize the way we approach player recruitment,” Zamora says. “It permits the identification of players based on the opinion of football experts embedded in the scouting reports and expressed in natural language. That is, we use the technology to fully extract the value and knowledge of our scouting department.”  And with the time saved, scouts can now concentrate on human tasks: connecting with recruits, watching games and making decisions backed by data. When considering the high functionality of Scout Advisor’s NLP technology, it’s natural to think about how the same technology can be applied to other sports recruiting and other functions. But one thing is certain: making better decisions about who, when and why to play a footballer has transformed the way Sevilla FC recruits.  Says Zamora: “This is the most revolutionary technology I have seen in football.”  ""How to establish lineage transparency for your machine learning initiatives -- Machine learning (ML) has become a critical component of many organizations’ digital transformation strategy. From predicting customer behavior to optimizing business processes, ML algorithms are increasingly being used to make decisions that impact business outcomes. Have you ever wondered how these algorithms arrive at their conclusions? The answer lies in the data used to train these models and how that data is derived. In this blog post, we will explore the importance of lineage transparency for machine learning data sets and how it can help establish and ensure, trust and reliability in ML conclusions. Trust in data is a critical factor for the success of any machine learning initiative. Executives evaluating decisions made by ML algorithms need to have faith in the conclusions they produce. After all, these decisions can have a significant impact on business operations, customer satisfaction and revenue. But trust isn’t important only for executives; before executive trust can be established, data scientists and citizen data scientists who create and work with ML models must have faith in the data they’re using. Understanding the meaning, quality and origins of data are the key factors in establishing trust. In this discussion we are focused on data origins and lineage.  Lineage describes the ability to track the origin, history, movement and transformation of data throughout its lifecycle. In the context of ML, lineage transparency means tracing the source of the data used to train any model understanding how that data is being transformed and identifying any potential biases or errors that may have been introduced along the way. The benefits of lineage transparency There are several benefits to implementing lineage transparency in ML data sets. Here are a few: Improved model performance: By understanding the origin and history of the data used to train ML models, data scientists can identify potential biases or errors that may impact model performance. This can lead to more accurate predictions and better decision-making. Increased trust: Lineage transparency can help establish trust in ML conclusions by providing a clear understanding of how the data was sourced, transformed and used to train models. This can be particularly important in industries where data privacy and security are paramount, such as healthcare and finance. Lineage details are also required for meeting regulatory guidelines. Faster troubleshooting: When issues arise with ML models, lineage transparency can help data scientists quickly identify the source of the problem. This can save time and resources by reducing the need for extensive testing and debugging. Improved collaboration: Lineage transparency facilitates collaboration and cooperation between data scientists and other stakeholders by providing a clear understanding of how data is being utilized. This leads to better communication, improved model performance and increased trust in the overall ML process. So how can organizations implement lineage transparency for their ML data sets? Let’s look at several strategies: Take advantage of data catalogs: Data catalogs are centralized repositories that provide a list of available data assets and their associated metadata. This can help data scientists understand the origin, format and structure of the data used to train ML models. Equally important is the fact that catalogs are also designed to identify data stewards—subject matter experts on particular data items—and also enable enterprises to define data in ways that everyone in the business can understand. Employ solid code management strategies: Version control systems like Git can help track changes to data and code over time. This code is often the true source of record for how data has been transformed as it weaves its way into ML training data sets. Make it a required practice to document all data sources: Documenting data sources and providing clear descriptions of how data has been transformed can help establish trust in ML conclusions. This can also make it easier for data scientists to understand how data is being used and identify potential biases or errors. This is critical for source data that is provided ad hoc or is managed by nonstandard or customized systems. Implement data lineage tooling and methodologies: Tools are available that help organizations track the lineage of their data sets from ultimate source to target by parsing code, ETL (extract, transform, load) solutions and more. These tools provide a visual representation of how data has been transformed and used to train models and also facilitate deep inspection of data pipelines. In conclusion, lineage transparency is a critical component of successful machine learning initiatives. By providing a clear understanding of how data is sourced, transformed and used to train models, organizations can establish trust in their ML results and ensure the performance of their models. Implementing lineage transparency can seem daunting, but there are several strategies and tools available to help organizations achieve this goal. By leveraging code management, data catalogs, data documentation and lineage tools, organizations can create a transparent and trustworthy data environment that supports their ML initiatives. With lineage transparency in place, data scientists can collaborate more effectively, troubleshoot issues more efficiently and improve model performance. Ultimately, lineage transparency is not just a nice-to-have, it’s a must-have for organizations that want to realize the full potential of their ML initiatives. If you are looking to take your ML initiatives to the next level, start by implementing data lineage for all your data pipelines. Your data scientists, executives and customers will thank you!""A new era in BI: Overcoming low adoption to make smart decisions accessible for all -- Organizations today are both empowered and overwhelmed by data. This paradox lies at the heart of modern business strategy: while there’s an unprecedented amount of data available, unlocking actionable insights requires more than access to numbers. The push to enhance productivity, use resources wisely, and boost sustainability through data-driven decision-making is stronger than ever. Yet, the low adoption rates of business intelligence (BI) tools present a significant hurdle. According to Gartner, although the number of employees that use analytics and business intelligence (ABI) has increased in 87% of surveyed organizations, ABI is still used by only 29% of employees on average. Despite the clear benefits of BI, the percentage of employees actively using ABI tools has seen minimal growth over the past 7 years. So why aren’t more people using BI tools? Understanding the low adoption rate: The low adoption rate of traditional BI tools, particularly dashboards, is a multifaceted issue rooted in both the inherent limitations of these tools and the evolving needs of modern businesses. Here’s a deeper look into why these challenges might persist and what it means for users across an organization: 1. Complexity and lack of accessibility. While excellent for displaying consolidated data views, dashboards often present a steep learning curve. This complexity makes them less accessible to nontechnical users, who might find these tools intimidating or overly complex for their needs. Moreover, the static nature of traditional dashboards means they are not built to adapt quickly to changes in data or business conditions without manual updates or redesigns. 2. Limited scope for actionable insights. Dashboards typically provide high-level summaries or snapshots of data, which are useful for quick status checks but often insufficient for making business decisions. They tend to offer limited guidance on what actions to take next, lacking the context needed to derive actionable, decision-ready insights. This can leave decision-makers feeling unsupported, as they need more than just data; they need insights that directly inform action. 3. The “unknown unknowns”. A significant barrier to BI adoption is the challenge of not knowing what questions to ask or what data might be relevant. Dashboards are static and require users to come with specific queries or metrics in mind. Without knowing what to look for, business analysts can miss critical insights, making dashboards less effective for exploratory data analysis and real-time decision-making. Moving beyond one-size-fits-all: The evolution of dashboards: While traditional dashboards have served us well, they are no longer sufficient on their own. The world of BI is shifting toward integrated and personalized tools that understand what each user needs. This isn’t just about being user-friendly; it’s about making these tools vital parts of daily decision-making processes for everyone, not just for those with technical expertise. Emerging technologies such as generative AI (gen AI) are enhancing BI tools with capabilities that were once only available to data professionals. These new tools are more adaptive, providing personalized BI experiences that deliver contextually relevant insights users can trust and act upon immediately. We’re moving away from the one-size-fits-all approach of traditional dashboards to more dynamic, customized analytics experiences. These tools are designed to guide users effortlessly from data discovery to actionable decision-making, enhancing their ability to act on insights with confidence. The future of BI: Making advanced analytics accessible to all: As we look toward the future, ease of use and personalization are set to redefine the trajectory of BI. 1. Emphasizing ease of use. The new generation of BI tools breaks down the barriers that once made powerful data analytics accessible only to data scientists. With simpler interfaces that include conversational interfaces, these tools make interacting with data as easy as having a chat. This integration into daily workflows means that advanced data analysis can be as straightforward as checking your email. This shift democratizes data access and empowers all team members to derive insights from data, regardless of their technical skills. For example, imagine a sales manager who wants to quickly check the latest performance figures before a meeting. Instead of navigating through complex software, they ask the BI tool, “What were our total sales last month?” or “How are we performing compared to the same period last year?” The system understands the questions and provides accurate answers in seconds, just like a conversation. This ease of use helps to ensure that every team member, not just data experts, can engage with data effectively and make informed decisions swiftly. 2. Driving personalization Personalization is transforming how BI platforms present and interact with data. It means that the system learns from how users work with it, adapting to suit individual preferences and meeting the specific needs of their business. For example, a dashboard might display the most important metrics for a marketing manager differently than for a production supervisor. It’s not just about the user’s role; it’s also about what’s happening in the market and what historical data shows. Alerts in these systems are also smarter. Rather than notifying users about all changes, the systems focus on the most critical changes based on past importance. These alerts can even adapt when business conditions change, helping to ensure that users get the most relevant information without having to look for it themselves. By integrating a deep understanding of both the user and their business environment, BI tools can offer insights that are exactly what’s needed at the right time. This makes these tools incredibly effective for making informed decisions quickly and confidently. Navigating the future: Overcoming adoption challenges. While the advantages of integrating advanced BI technologies are clear, organizations often encounter significant challenges that can hinder their adoption. Understanding these challenges is crucial for businesses looking to use the full potential of these innovative tools. 1. Cultural resistance to change. One of the biggest hurdles is overcoming ingrained habits and resistance within the organization. Employees used to traditional methods of data analysis might be skeptical about moving to new systems, fearing the learning curve or potential disruptions to their routine workflows. Promoting a culture that values continuous learning and technological adaptability is key to overcoming this resistance. 2. Complexity of integration. Integrating new BI technologies with existing IT infrastructure can be complex and costly. Organizations must help ensure that new tools are compatible with their current systems, which often involve significant time and technical expertise. The complexity increases when trying to maintain data consistency and security across multiple platforms. 3. Data governance and security. Gen AI, by its nature, creates new content based on existing data sets. The outputs generated by AI can sometimes introduce biases or inaccuracies if not properly monitored and managed. With the increased use of AI and machine learning in BI tools, managing data privacy and security becomes more complex. Organizations must help ensure that their data governance policies are robust enough to handle new types of data interactions and comply with regulations such as GDPR. This often requires updating security protocols and continuously monitoring data access and usage. According to Gartner, by 2025, augmented consumerization functions will drive the adoption of ABI capabilities beyond 50% for the first time, influencing more business processes and decisions. As we stand on the brink of this new era in BI, we must focus on adopting new technologies and managing them wisely. By fostering a culture that embraces continuous learning and innovation, organizations can fully harness the potential of gen AI and augmented analytics to make smarter, faster and more informed decisions.""Prioritizing operational resiliency to reduce downtime in payments -- The average lost business cost following a data breach was USD 1.3 million in 2023, according to IBM’s Cost of a Data Breach report. With the rapid emergence of real-time payments, any downtime in payments connectivity can be a significant threat. This downtime can harm a business’s reputation, as well as the global financial ecosystem. For this reason, it’s paramount that financial enterprises support their resiliency needs by adopting a robust infrastructure that is integrated across multiple environments, including the cloud, on prem and at the edge. Resiliency helps financial institutions build customer and regulator confidence - Retaining customers is crucial to any business strategy, and maintaining customer trust is key to a financial institution’s success. We believe enterprises that prioritize resilience demonstrate their commitment to providing their consumers with a seamless experience in the event of disruption. In addition to maintaining customer trust, financial enterprises must maintain regulator trust as well. Regulations around the world, such as the Digital Operational Resilience Act (DORA), continue to grow. DORA is a European Union regulation that aims to establish technical standards that financial entities and their critical third-party technology service providers must implement in their ICT systems by 17 January 2025. DORA requires financial institutions to define the business recovery process, service levels and recovery times that are acceptable for their business across processes, including payments. Traditionally, this has caused covered institutions to evaluate their cybersecurity protection measures. To meet customer and regulator demands, it is critical that financial institutions are proactive and strategic about creating a cohesive strategy to modernize their payments infrastructure with resiliency and compliance at the forefront. How IBM helps clients address resiliency in payments - As the need for operational resilience grows, enterprises increasingly adopt hybrid cloud strategies to store their data across multiple environments including the cloud, on prem and at the edge. By developing a workload placement strategy based on the uniqueness of a financial entity’s business processes and applications, they can optimize the output of these applications to enable the continuation of services 24/7. IBM Cloud® remains committed to providing our clients with an enterprise-grade cloud platform that can help them address resiliency, performance, security and compliance obligations. IBM Cloud also supports mission-critical workloads and addresses evolving regulations around the globe. To accelerate cloud adoption in financial services, we built IBM Cloud for Financial Services®, informed by the industry and for the industry. With security controls built into the platform, we aim to help financial entities minimize risk as they maintain and demonstrate their compliance with their regulators. With approximately 500 industry practitioners across the globe, the expertise of the IBM Payments Center® provides clients with guidance on their end-to-end payments’ modernization journey. Also, clients can use payments as a service, including checks as a service, which can help give them access to the benefits of a managed, secured cloud-based platform that can scale up and down to meet changing electronic payment and check volumes. IBM’s swift connectivity capabilities on IBM Cloud for Financial Services enable resiliency and use IBM Cloud multizone regions to help keep data secured and enable business continuity in case of advanced ransomware or cyberattacks. IBM® can help you navigate the highly interconnected payments ecosystem and build resiliency. Partner with us to reduce downtime, protect your reputation and maintain the trust of your customers and regulators.""Maximizing SaaS application analytics value with AI -- Software as a service (SaaS) applications have become a boon for enterprises looking to maximize network agility while minimizing costs. They offer app developers on-demand scalability and faster time-to-benefit for new features and software updates. SaaS takes advantage of cloud computing infrastructure and economies of scale to provide clients a more streamlined approach to adopting, using and paying for software. However, SaaS architectures can easily overwhelm DevOps teams with data aggregation, sorting and analysis tasks. Given the volume of SaaS apps on the market (more than 30,000 SaaS developers were operating in 2023) and the volume of data a single app can generate (with each enterprise businesses using roughly 470 SaaS apps), SaaS leaves businesses with loads of structured and unstructured data to parse. That’s why today’s application analytics platforms rely on artificial intelligence (AI) and machine learning (ML) technology to sift through big data, provide valuable business insights and deliver superior data observability. What is application analytics? Broadly speaking, application analytics refers to the process of collecting application data and performing real-time analysis of SaaS, mobile, desktop and web application performance and usage data. App analytics include: App usage analytics, which show app usage patterns (such as daily and monthly active users, most- and least-used features and geographical distribution of downloads). App performance analytics, which show how apps are performing across the network (with metrics such as response times and failure rates) and identify the cause and location of app, server or network problems. App cost and revenue analytics, which track app revenue—such as annual recurring revenue and customer lifetime value (the total profit a business can expect to make from a single customer for the duration the business relationship)—and expenditures such as customer acquisition cost (the costs associated with acquiring a new customer). Using sophisticated data visualization tools, many of which are powered by AI, app analytics services empower businesses to better understand IT operations, helping teams make smarter decisions, faster. AI in SaaS analytics - Most industries have had to reckon with AI proliferation and AI-driven business practices to some extent. Roughly 42% of enterprise-scale organizations (more than 1,000 employees) have used AI for business purposes, with nearly 60% of enterprises already using AI to accelerate tech investment. And by 2026, more than 80% of companies will have deployed AI) )AI-enabled apps in their IT environments (up from only 5% in 2023). SaaS app development and management is no different. SaaS offers businesses cloud-native app capabilities, but AI and ML turn the data generated by SaaS apps into actionable insights. Modern SaaS analytics solutions can seamlessly integrate with AI models to predict user behavior and automate data sorting and analysis; and ML algorithms enable SaaS apps to learn and improve over time. Using comprehensive, AI-driven SaaS analytics, businesses can make data-driven decisions about feature enhancements, UI/UX improvements and marketing strategies to maximize user engagement and meet—or exceed—business goals. SaaS app analytics use cases - While effective for some organizations, traditional SaaS data analysis methods (such as relying solely on human data analysts to aggregate data points) sometimes fall short in handling the massive quantities of data SaaS apps produce. They may also struggle to fully leverage the predictive capabilities of app analytics. The introduction of AI and ML technologies, however, can provide more nuanced observability and more effective decision automation. AI- and ML-generated SaaS analytics enhance: 1. Data insights and reporting - Application analytics help businesses monitor key performance indicators (KPIs)—such as error rates, response time, resource utilization, user retention and dependency rates, among other key metrics—to identify performance issues and bottlenecks and create a smoother user experience. AI and ML algorithms enhance these features by processing unique app data more efficiently. AI technologies can also reveal and visualize data patterns to help with feature development. If, for instance, a development team wants to understand which app features most significantly impact retention, it might use AI-driven natural language processing (NLP) to analyze unstructured data. NLP protocols will auto-categorize user-generated content (such as customer reviews and support tickets), summarize the data and offer insights into the features that keep customers returning to the app. AI can even use NLP to suggest new tests, algorithms, lines of code or entirely new app functions to increase retention. With AI and ML algorithms, SaaS developers also get granular observability into app analytics. AI-powered analytics programs can create real-time, fully customizable dashboards that provide up-to-the-minute insights into KPIs. And most machine learning tools will automatically generate summaries of complex data, making it easier for executives and other decision-makers to understand reports without needing to review the raw data themselves. 2. Predictive analytics - Predictive analytics forecast future events based on historical data; AI and ML models—such as regression analysis, neural networks and decision trees—enhance the accuracy of these predictions. An e-commerce app, for example, can predict which products will be popular during the holidays by analyzing historical purchase data from previous holiday seasons. Most SaaS analytics tools—including Google Analytics, Microsoft Azure and IBM® Instana®—offer predictive analytics features that enable developers to anticipate both market and user behavior trends  and shift their business strategy accordingly.  Predictive analytics are equally valuable for user insights. AI and ML features enable SaaS analytics software to run complex analyses of user interactions within the app (click patterns, navigation paths, feature usage and session duration, among other metrics), which ultimately helps teams anticipate user behavior. For instance, if a company wants to implement churn prediction protocols to identify at-risk users, they can use AI functions to analyze activity reduction and negative feedback patterns, two user engagement metrics that often precede churn. After the program identifies at-risk users, machine learning algorithms can suggest personalized interventions to re-engage them (a subscription service might offer discounted or exclusive content to users showing signs of disengagement). Diving deeper into user behavior data also helps businesses proactively identify app usability issues. And during unexpected disruptions (such as those caused by a natural disaster), AI and SaaS analytics provide real-time data visibility that keeps businesses running—or even improving—in challenging times. 3. Personalization and user experience optimization - Machine learning technologies are often integral to providing a personalized customer experience in SaaS applications. Using customer preferences (preferred themes, layouts and functions), historical trends and user interaction data, ML models in SaaS can dynamically tailor the content that users see based on real-time data. In other words, AI-powered SaaS apps can automatically implement adaptive interface design to keep users engaged with personalized recommendations and content experiences. News apps, for instance, can highlight articles similar to the ones a user has previously read and liked. An online learning platform can recommend courses or onboarding steps based on a user’s learning history and preferences. And notification systems can send targeted messages to each user at the time they’re likeliest to engage, making the overall experience more relevant and enjoyable. At the application level, AI can analyze user journey data to understand the typical navigation paths users take through the app and streamline navigation for the entire user base. 4. Conversion rate optimization and marketing - AI analytics tools offer businesses the opportunity to optimize conversion rates, whether through form submissions, purchases, sign-ups or subscriptions. AI-based analytics programs can automate funnel analyses (which identify where in the conversion funnel users drop off), A/B tests (where developers test multiple design elements, features or conversion paths to see which performs better) and call-to-action button optimization to increase conversions. Data insights from AI and ML also help improve product marketing and increase overall app profitability, both vital components to maintaining SaaS applications. Companies can use AI to automate tedious marketing tasks (such as lead generation and ad targeting), maximizing both advertising ROI and conversation rates. And with ML features, developers can track user activity to more accurately segment and sell products to the user base (with conversion incentives, for instance).  5. Pricing optimization - Managing IT infrastructure can be an expensive undertaking, especially for an enterprise running a large network of cloud-native applications. AI and ML features help minimize cloud expenditures (and cloud waste) by automating SaaS process responsibilities and streamlining workflows. Using AI-generated predictive analytics and real-time financial observability tools, teams can anticipate resource usage fluctuations and allocate network resources accordingly. SaaS analytics also enable decision-makers to identify underutilized or problematic assets, preventing over- and under-spending and freeing up capital for app innovations and improvements. Maximize the value of SaaS analytics data with IBM Instana Observability - AI-powered application analytics give developers an advantage in today’s fast-paced, hyper-dynamic SaaS landscape, and with IBM Instana, businesses can get an industry-leading, real-time, full-stack observability solution. Instana is more than a traditional app performance management (APM) solution. It provides automated, democratized observability with AI, making it accessible to anyone across DevOps, SRE, platform engineering, ITOps and development. Instana gives companies the data that they want—with the context that they need—to take intelligent action and maximize the potential of SaaS app analytics."