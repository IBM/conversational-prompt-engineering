text"How AI-powered recruiting helps Spain’s leading soccer team score. Phrases like “striking the post” and “direct free kick outside the 18” may seem foreign if you’re not a fan of football (for Americans, see: soccer). But for a football scout, it’s the daily lexicon of the job, representing crucial language that helps assess a player’s value to a team. And now, it’s also the language spoken and understood by Scout Advisor—an innovative tool using natural language processing (NLP) and built on the IBM® watsonx™ platform especially for Spain’s Sevilla Fútbol Club. On any given day, a scout has several responsibilities: observing practices, talking to families of young players, taking notes on games and recording lots of follow-up paperwork. In fact, paperwork is a much more significant part of the job than one might imagine. As Victor Orta, Sevilla FC Sporting Director, explained at his conference during the World Football Summit in 2023: “We are never going to sign a player with data alone, but we will never do it without resorting to data either. In the end, the good player will always have good data, but then there is always the human eye, which is the one that must evaluate everything and decide.” Benched by paperwork: Back in 2021, an avalanche of paperwork plagued Sevilla FC, a top-flight team based in Andalusia, Spain. With an elite scouting team featuring 20-to-25 scouts, a single player can accumulate up to 40 scout reports, requiring 200-to-300 hours of review. Overall, Sevilla FC was tasked with organizing more than 200,000 total reports on potential players—an immensely time-consuming job. Combining expert observation alongside the value of data remained key for the club. Scout reports look at the quantitative data of game-time minutiae, like scoring attempts, accurate pass percentages, assists, as well as qualitative data like a player’s attitude and alignment with team philosophy. At the time, Sevilla FC could efficiently access and use quantitative player data in a matter of seconds, but the process of extracting qualitative information from the database was much slower in comparison. In the case of Sevilla FC, using big data to recruit players had the potential to change the core business. Instead of scouts choosing players based on intuition and bias alone, they could also use statistics, and confidently make better business decisions on multi-million-dollar investments (that is, players). Not to mention, when, where and how to use said players. But harnessing that data was no easy task. Getting the IBM assist: Sevilla FC takes data almost as seriously as scoring goals. In 2021, the club created a dedicated data department specifically to help management make better business decisions. It has now grown to be the largest data department in European football, developing its own AI tool to help track player movements through news coverage, as well as internal ticketing solutions. But when it came to the massive amount of data collected by scouters, the department knew it had a challenge that would take a reliable partner. Initially, the department consulted with data scientists at the University of Sevilla to develop models to organize all their data. But soon, the club realized it would need more advanced technology. A cold call from an IBM representative was fortuitous. “I was contacted by [IBM Client Engineering Manager] Arturo Guerrero to know more about us and our data projects,” says Elias Zamora, Sevilla FC chief data officer. “We quickly understood there were ways to cooperate. Sevilla FC has one of the biggest scouting databases in the professional football, ready to be used in the framework of generative AI technologies. IBM had just released watsonx, its commercial generative AI and scientific data platform based on cloud. Therefore, a partnership to extract the most value from our scouting reports using AI was the right initiative.”  Coordinating the play: Sevilla FC connected with the IBM Client Engineering team to talk through its challenges and a plan was devised. Because Sevilla FC was able to clearly explain its challenges and goals—and IBM asked the right questions—the technology soon followed. The partnership determined that IBM watsonx.ai™ would be the best solution to quickly and easily sift through a massive player database using foundation models and generative AI to process prompts in natural language. Using semantic language for search provided richer results: for instance, a search for “talented winger” translated to “a talented winger is capable of taking on defenders with dribbling to create space and penetrate the opposition’s defense.”  The solution—titled Scout Advisor—presents a curated list of players matching search criteria in a well-designed, user-friendly interface. Its technology helps unlock the entire potential of the Sevilla FC’s database, from the intangible impressions of a scout to specific data assets. Scoring the goal : Scout Advisor’s pilot program went into production in January 2024, and is currently training with 200,000 existing reports. The club’s plan is to use the tool during the summer 2024 recruiting season and see results in September. So far, the reviews have been positive.  “Scout Advisor has the capability to revolutionize the way we approach player recruitment,” Zamora says. “It permits the identification of players based on the opinion of football experts embedded in the scouting reports and expressed in natural language. That is, we use the technology to fully extract the value and knowledge of our scouting department.”  And with the time saved, scouts can now concentrate on human tasks: connecting with recruits, watching games and making decisions backed by data. When considering the high functionality of Scout Advisor’s NLP technology, it’s natural to think about how the same technology can be applied to other sports recruiting and other functions. But one thing is certain: making better decisions about who, when and why to play a footballer has transformed the way Sevilla FC recruits.  Says Zamora: “This is the most revolutionary technology I have seen in football.”  ""How to establish lineage transparency for your machine learning initiatives. Machine learning (ML) has become a critical component of many organizations’ digital transformation strategy. From predicting customer behavior to optimizing business processes, ML algorithms are increasingly being used to make decisions that impact business outcomes. Have you ever wondered how these algorithms arrive at their conclusions? The answer lies in the data used to train these models and how that data is derived. In this blog post, we will explore the importance of lineage transparency for machine learning data sets and how it can help establish and ensure, trust and reliability in ML conclusions. Trust in data is a critical factor for the success of any machine learning initiative. Executives evaluating decisions made by ML algorithms need to have faith in the conclusions they produce. After all, these decisions can have a significant impact on business operations, customer satisfaction and revenue. But trust isn’t important only for executives; before executive trust can be established, data scientists and citizen data scientists who create and work with ML models must have faith in the data they’re using. Understanding the meaning, quality and origins of data are the key factors in establishing trust. In this discussion we are focused on data origins and lineage.  Lineage describes the ability to track the origin, history, movement and transformation of data throughout its lifecycle. In the context of ML, lineage transparency means tracing the source of the data used to train any model understanding how that data is being transformed and identifying any potential biases or errors that may have been introduced along the way. The benefits of lineage transparency There are several benefits to implementing lineage transparency in ML data sets. Here are a few: Improved model performance: By understanding the origin and history of the data used to train ML models, data scientists can identify potential biases or errors that may impact model performance. This can lead to more accurate predictions and better decision-making. Increased trust: Lineage transparency can help establish trust in ML conclusions by providing a clear understanding of how the data was sourced, transformed and used to train models. This can be particularly important in industries where data privacy and security are paramount, such as healthcare and finance. Lineage details are also required for meeting regulatory guidelines. Faster troubleshooting: When issues arise with ML models, lineage transparency can help data scientists quickly identify the source of the problem. This can save time and resources by reducing the need for extensive testing and debugging. Improved collaboration: Lineage transparency facilitates collaboration and cooperation between data scientists and other stakeholders by providing a clear understanding of how data is being utilized. This leads to better communication, improved model performance and increased trust in the overall ML process. So how can organizations implement lineage transparency for their ML data sets? Let’s look at several strategies: Take advantage of data catalogs: Data catalogs are centralized repositories that provide a list of available data assets and their associated metadata. This can help data scientists understand the origin, format and structure of the data used to train ML models. Equally important is the fact that catalogs are also designed to identify data stewards—subject matter experts on particular data items—and also enable enterprises to define data in ways that everyone in the business can understand. Employ solid code management strategies: Version control systems like Git can help track changes to data and code over time. This code is often the true source of record for how data has been transformed as it weaves its way into ML training data sets. Make it a required practice to document all data sources: Documenting data sources and providing clear descriptions of how data has been transformed can help establish trust in ML conclusions. This can also make it easier for data scientists to understand how data is being used and identify potential biases or errors. This is critical for source data that is provided ad hoc or is managed by nonstandard or customized systems. Implement data lineage tooling and methodologies: Tools are available that help organizations track the lineage of their data sets from ultimate source to target by parsing code, ETL (extract, transform, load) solutions and more. These tools provide a visual representation of how data has been transformed and used to train models and also facilitate deep inspection of data pipelines. In conclusion, lineage transparency is a critical component of successful machine learning initiatives. By providing a clear understanding of how data is sourced, transformed and used to train models, organizations can establish trust in their ML results and ensure the performance of their models. Implementing lineage transparency can seem daunting, but there are several strategies and tools available to help organizations achieve this goal. By leveraging code management, data catalogs, data documentation and lineage tools, organizations can create a transparent and trustworthy data environment that supports their ML initiatives. With lineage transparency in place, data scientists can collaborate more effectively, troubleshoot issues more efficiently and improve model performance. Ultimately, lineage transparency is not just a nice-to-have, it’s a must-have for organizations that want to realize the full potential of their ML initiatives. If you are looking to take your ML initiatives to the next level, start by implementing data lineage for all your data pipelines. Your data scientists, executives and customers will thank you!""A new era in BI: Overcoming low adoption to make smart decisions accessible for all. Organizations today are both empowered and overwhelmed by data. This paradox lies at the heart of modern business strategy: while there’s an unprecedented amount of data available, unlocking actionable insights requires more than access to numbers. The push to enhance productivity, use resources wisely, and boost sustainability through data-driven decision-making is stronger than ever. Yet, the low adoption rates of business intelligence (BI) tools present a significant hurdle. According to Gartner, although the number of employees that use analytics and business intelligence (ABI) has increased in 87% of surveyed organizations, ABI is still used by only 29% of employees on average. Despite the clear benefits of BI, the percentage of employees actively using ABI tools has seen minimal growth over the past 7 years. So why aren’t more people using BI tools? Understanding the low adoption rate: The low adoption rate of traditional BI tools, particularly dashboards, is a multifaceted issue rooted in both the inherent limitations of these tools and the evolving needs of modern businesses. Here’s a deeper look into why these challenges might persist and what it means for users across an organization: 1. Complexity and lack of accessibility. While excellent for displaying consolidated data views, dashboards often present a steep learning curve. This complexity makes them less accessible to nontechnical users, who might find these tools intimidating or overly complex for their needs. Moreover, the static nature of traditional dashboards means they are not built to adapt quickly to changes in data or business conditions without manual updates or redesigns. 2. Limited scope for actionable insights. Dashboards typically provide high-level summaries or snapshots of data, which are useful for quick status checks but often insufficient for making business decisions. They tend to offer limited guidance on what actions to take next, lacking the context needed to derive actionable, decision-ready insights. This can leave decision-makers feeling unsupported, as they need more than just data; they need insights that directly inform action. 3. The “unknown unknowns”. A significant barrier to BI adoption is the challenge of not knowing what questions to ask or what data might be relevant. Dashboards are static and require users to come with specific queries or metrics in mind. Without knowing what to look for, business analysts can miss critical insights, making dashboards less effective for exploratory data analysis and real-time decision-making. Moving beyond one-size-fits-all: The evolution of dashboards: While traditional dashboards have served us well, they are no longer sufficient on their own. The world of BI is shifting toward integrated and personalized tools that understand what each user needs. This isn’t just about being user-friendly; it’s about making these tools vital parts of daily decision-making processes for everyone, not just for those with technical expertise. Emerging technologies such as generative AI (gen AI) are enhancing BI tools with capabilities that were once only available to data professionals. These new tools are more adaptive, providing personalized BI experiences that deliver contextually relevant insights users can trust and act upon immediately. We’re moving away from the one-size-fits-all approach of traditional dashboards to more dynamic, customized analytics experiences. These tools are designed to guide users effortlessly from data discovery to actionable decision-making, enhancing their ability to act on insights with confidence. The future of BI: Making advanced analytics accessible to all: As we look toward the future, ease of use and personalization are set to redefine the trajectory of BI. 1. Emphasizing ease of use. The new generation of BI tools breaks down the barriers that once made powerful data analytics accessible only to data scientists. With simpler interfaces that include conversational interfaces, these tools make interacting with data as easy as having a chat. This integration into daily workflows means that advanced data analysis can be as straightforward as checking your email. This shift democratizes data access and empowers all team members to derive insights from data, regardless of their technical skills. For example, imagine a sales manager who wants to quickly check the latest performance figures before a meeting. Instead of navigating through complex software, they ask the BI tool, “What were our total sales last month?” or “How are we performing compared to the same period last year?” The system understands the questions and provides accurate answers in seconds, just like a conversation. This ease of use helps to ensure that every team member, not just data experts, can engage with data effectively and make informed decisions swiftly. 2. Driving personalization Personalization is transforming how BI platforms present and interact with data. It means that the system learns from how users work with it, adapting to suit individual preferences and meeting the specific needs of their business. For example, a dashboard might display the most important metrics for a marketing manager differently than for a production supervisor. It’s not just about the user’s role; it’s also about what’s happening in the market and what historical data shows. Alerts in these systems are also smarter. Rather than notifying users about all changes, the systems focus on the most critical changes based on past importance. These alerts can even adapt when business conditions change, helping to ensure that users get the most relevant information without having to look for it themselves. By integrating a deep understanding of both the user and their business environment, BI tools can offer insights that are exactly what’s needed at the right time. This makes these tools incredibly effective for making informed decisions quickly and confidently. Navigating the future: Overcoming adoption challenges. While the advantages of integrating advanced BI technologies are clear, organizations often encounter significant challenges that can hinder their adoption. Understanding these challenges is crucial for businesses looking to use the full potential of these innovative tools. 1. Cultural resistance to change. One of the biggest hurdles is overcoming ingrained habits and resistance within the organization. Employees used to traditional methods of data analysis might be skeptical about moving to new systems, fearing the learning curve or potential disruptions to their routine workflows. Promoting a culture that values continuous learning and technological adaptability is key to overcoming this resistance. 2. Complexity of integration. Integrating new BI technologies with existing IT infrastructure can be complex and costly. Organizations must help ensure that new tools are compatible with their current systems, which often involve significant time and technical expertise. The complexity increases when trying to maintain data consistency and security across multiple platforms. 3. Data governance and security. Gen AI, by its nature, creates new content based on existing data sets. The outputs generated by AI can sometimes introduce biases or inaccuracies if not properly monitored and managed. With the increased use of AI and machine learning in BI tools, managing data privacy and security becomes more complex. Organizations must help ensure that their data governance policies are robust enough to handle new types of data interactions and comply with regulations such as GDPR. This often requires updating security protocols and continuously monitoring data access and usage. According to Gartner, by 2025, augmented consumerization functions will drive the adoption of ABI capabilities beyond 50% for the first time, influencing more business processes and decisions. As we stand on the brink of this new era in BI, we must focus on adopting new technologies and managing them wisely. By fostering a culture that embraces continuous learning and innovation, organizations can fully harness the potential of gen AI and augmented analytics to make smarter, faster and more informed decisions."